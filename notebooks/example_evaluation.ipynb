{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with `justcause`\n",
    "\n",
    "In this notebook, we examplify how to use `justcause` in order to evaluate methods using reference datasets. For simplicity, we only use one dataset, but show how evaluation works with multiple methods. Both standard causal methods implemented in the framework as well as custom methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom First\n",
    "The goal of the `justcause` framework is to be a modular and flexible facilitator of causal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# Loading all required packages \n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from justcause.data import Col\n",
    "from justcause.data.sets import load_ihdp\n",
    "from justcause.metrics import pehe_score, mean_absolute\n",
    "from justcause.evaluation import setup_result_df, setup_scores_df, calc_scores, \\\n",
    "    summarize_scores\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data and methods you want to evaluate\n",
    "Let's say we wanted to compare a S-Learner with propensity weighting, based on a propensity estimate of our choice. Thus, we cannot simply use the predefined SLearner from `justcause.learners`, but have to provide our own adaption, which first estimates propensities and uses these for fitting an adjusted model. \n",
    "\n",
    "By providing a \"blackbox\" method like below, you can choose to do whatever you want inside. For example, you can replace your predictions available factual outcomes, estimate the propensity in different ways or even use a true propensity, in case of a generated dataset, where it is available. You can also resort to out-of-sample prediction, where no information about treatment is provided to the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justcause.learners import SLearner\n",
    "from justcause.learners.propensity import estimate_propensities\n",
    "\n",
    "\n",
    "data = load_ihdp()\n",
    "metrics = [pehe_score, mean_absolute]\n",
    "\n",
    "# Limit evaluation to the first 100 replications of IHDP\n",
    "replications = list(itertools.islice(data, 100))\n",
    "train_size = 0.8\n",
    "random_state = 42\n",
    "\n",
    "def weighted_slearner(train, test):\n",
    "    \"\"\"\n",
    "    Custom method that takes 'train' and 'test' CausalFrames (see causal_frames.ipynb)\n",
    "    and returns ITE predictions for both after training on 'train'. \n",
    "    \n",
    "    Implement your own method in a similar fashion to evaluate them within the framework!\n",
    "    \"\"\"\n",
    "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
    "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
    "    \n",
    "    \n",
    "    # Get calibrated propensity estimates\n",
    "    p = estimate_propensities(train_X, train_t)\n",
    "\n",
    "    # Make sure the supplied learner is able to use `sample_weights` in the fit() method\n",
    "    slearner = SLearner(LinearRegression())\n",
    "    \n",
    "    # Weight with inverse probability of treatment (inverse propensity)\n",
    "    slearner.fit(train_X, train_t, train_y, weights=1/p)\n",
    "    return (\n",
    "        slearner.predict_ite(train_X, train_t, train_y),\n",
    "        slearner.predict_ite(test_X, test_t, test_y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Evaluation Loop\n",
    "Now given a callable like `weighted_slearner` we can evaluate that method using multiple metrics on the given replications. \n",
    "The result dataframe then contains two rows with the summarized scores over all replications for train and test separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = setup_result_df(metrics)\n",
    "    \n",
    "test_scores = setup_scores_df(metrics)\n",
    "train_scores = setup_scores_df(metrics)\n",
    "\n",
    "for rep in replications:\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        rep, train_size=train_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # REPLACE this with the function you implemented and want to evaluate\n",
    "    train_ite, test_ite = weighted_slearner(train, test)\n",
    "\n",
    "    # Calculate the scores and append them to a dataframe\n",
    "    test_scores.loc[len(test_scores)] = calc_scores(test[Col.ite],\n",
    "                                                    test_ite,\n",
    "                                                    metrics)\n",
    "\n",
    "    train_scores.loc[len(train_scores)] = calc_scores(train[Col.ite],\n",
    "                                                    train_ite,\n",
    "                                                    metrics)b\n",
    "\n",
    "# Summarize the scores and save them in a dataframe\n",
    "results_df.loc[len(results_df)] = np.append(['slearner', True], summarize_scores(train_scores))\n",
    "results_df.loc[len(results_df)] = np.append(['slearner', False], summarize_scores(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slearner</td>\n",
       "      <td>True</td>\n",
       "      <td>5.592355721307152</td>\n",
       "      <td>2.5694717507476366</td>\n",
       "      <td>8.248291408843441</td>\n",
       "      <td>0.3699388743475459</td>\n",
       "      <td>0.2124273147540643</td>\n",
       "      <td>0.5243953093782159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slearner</td>\n",
       "      <td>False</td>\n",
       "      <td>5.493401193725237</td>\n",
       "      <td>2.589651399901557</td>\n",
       "      <td>7.903173959543428</td>\n",
       "      <td>0.6556018033508734</td>\n",
       "      <td>0.28720143217444916</td>\n",
       "      <td>0.9419412237403737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method  train  ... mean_absolute-median   mean_absolute-std\n",
       "0  slearner   True  ...   0.2124273147540643  0.5243953093782159\n",
       "1  slearner  False  ...  0.28720143217444916  0.9419412237403737\n",
       "\n",
       "[2 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this case, using `justcause` has hardly any advantages, because only one dataset and one method is used. You might as well just implement all the evaluation manually. However, this can simply be expanded to more methods by looping over the callables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_slearner(train, test):\n",
    "    \"\"\" \"\"\"\n",
    "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
    "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
    "\n",
    "    slearner = SLearner(LinearRegression())\n",
    "    slearner.fit(train_X, train_t, train_y)\n",
    "    return (\n",
    "        slearner.predict_ite(train_X, train_t, train_y),\n",
    "        slearner.predict_ite(test_X, test_t, test_y)\n",
    "    )\n",
    "\n",
    "methods = [basic_slearner, weighted_slearner]\n",
    "\n",
    "results_df = setup_result_df(metrics)\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    test_scores = setup_scores_df(metrics)\n",
    "    train_scores = setup_scores_df(metrics)\n",
    "\n",
    "    for rep in replications:\n",
    "\n",
    "        train, test = train_test_split(\n",
    "            rep, train_size=train_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # REPLACE this with the function you implemented and want to evaluate\n",
    "        train_ite, test_ite = method(train, test)\n",
    "\n",
    "        # Calculate the scores and append them to a dataframe\n",
    "        test_scores.loc[len(test_scores)] = calc_scores(test[Col.ite],\n",
    "                                                        test_ite,\n",
    "                                                        metrics)\n",
    "\n",
    "        train_scores.loc[len(train_scores)] = calc_scores(train[Col.ite],\n",
    "                                                        train_ite,\n",
    "                                                        metrics)\n",
    "\n",
    "    # Summarize the scores and save them in a dataframe\n",
    "    results_df.loc[len(results_df)] = np.append([method.__name__, True], summarize_scores(train_scores))\n",
    "    results_df.loc[len(results_df)] = np.append([method.__name__, False], summarize_scores(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic_slearner</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_slearner</td>\n",
       "      <td>False</td>\n",
       "      <td>5.625971000721637</td>\n",
       "      <td>2.6359926738390502</td>\n",
       "      <td>8.213625971533043</td>\n",
       "      <td>1.2926681149657069</td>\n",
       "      <td>0.39624557185266385</td>\n",
       "      <td>2.4746034286861276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_slearner</td>\n",
       "      <td>True</td>\n",
       "      <td>5.592355721307152</td>\n",
       "      <td>2.5694717507476366</td>\n",
       "      <td>8.248291408843441</td>\n",
       "      <td>0.3699388743475459</td>\n",
       "      <td>0.2124273147540643</td>\n",
       "      <td>0.5243953093782159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_slearner</td>\n",
       "      <td>False</td>\n",
       "      <td>5.493401193725237</td>\n",
       "      <td>2.589651399901557</td>\n",
       "      <td>7.903173959543428</td>\n",
       "      <td>0.6556018033508734</td>\n",
       "      <td>0.28720143217444916</td>\n",
       "      <td>0.9419412237403737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              method  train  ... mean_absolute-median   mean_absolute-std\n",
       "0     basic_slearner   True  ...  0.23818504313199274  1.4932757697867245\n",
       "1     basic_slearner  False  ...  0.39624557185266385  2.4746034286861276\n",
       "2  weighted_slearner   True  ...   0.2124273147540643  0.5243953093782159\n",
       "3  weighted_slearner  False  ...  0.28720143217444916  0.9419412237403737\n",
       "\n",
       "[4 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because in most cases, we're not changing anything within this loop for the ITE case, `justcause` provides a default implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Evaluation of ITE predictions\n",
    "Using the same list of method callables, we can just call `evaluate_ite` and pass all the information. The default implementation sets up a dataframe for the result following a certain convention. \n",
    "\n",
    "First, there's two columns to define the method for which the results are as well as whether they've been calculated on train or test. Then for all supplied `metrics`, all `formats` will be listed. \n",
    "\n",
    "Standard `metrics` like (PEHE or Mean absolute error) are implemented in `justcause.metrics`. \n",
    "Standard formats used for summarizing the scores over multiple replications are `np.mean, np.median, np.std`, other possibly interesting formats could be *skewness*, *minmax*, *kurtosis*. A method provided as format must take an `axis` parameter, ensuring that it can be applied to the scores dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justcause.evaluation import evaluate_ite\n",
    "\n",
    "result = evaluate_ite(replications, methods, metrics, train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic_slearner</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>basic_slearner</td>\n",
       "      <td>False</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_slearner</td>\n",
       "      <td>True</td>\n",
       "      <td>5.592355721307152</td>\n",
       "      <td>2.5694717507476366</td>\n",
       "      <td>8.248291408843441</td>\n",
       "      <td>0.3699388743475459</td>\n",
       "      <td>0.2124273147540643</td>\n",
       "      <td>0.5243953093782159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_slearner</td>\n",
       "      <td>False</td>\n",
       "      <td>5.592355721307152</td>\n",
       "      <td>2.5694717507476366</td>\n",
       "      <td>8.248291408843441</td>\n",
       "      <td>0.3699388743475459</td>\n",
       "      <td>0.2124273147540643</td>\n",
       "      <td>0.5243953093782159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              method  train  ... mean_absolute-median   mean_absolute-std\n",
       "0     basic_slearner   True  ...  0.23818504313199274  1.4932757697867245\n",
       "1     basic_slearner  False  ...  0.23818504313199274  1.4932757697867245\n",
       "2  weighted_slearner   True  ...   0.2124273147540643  0.5243953093782159\n",
       "3  weighted_slearner  False  ...   0.2124273147540643  0.5243953093782159\n",
       "\n",
       "[4 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding standard causal methods to the mix\n",
    "Within `justcause.learners` we've implemented a couple of standard methods that provide a `predict_ite()` method. Instead of going the tedious way like we've done in `weighted_slearner` above, we can just use these methods directly. The default implementation will use a default base learner for all the meta-learners, fit the method on train and predict the ITEs for train and test. \n",
    "\n",
    "By doing so, we can get rid of the `basic_slearner` method above, because it just uses the default setting and procedure for fitting the model. Instead, we just use `SLearner(LinearRegression())`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justcause.learners import TLearner, XLearner, RLearner\n",
    "\n",
    "# All in standard configuration\n",
    "methods = [SLearner(LinearRegression()), weighted_slearner, TLearner(), XLearner(), RLearner(LinearRegression())]\n",
    "\n",
    "result = evaluate_ite(replications, methods, metrics, train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
