{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running custom and standard evaluation with `justcause`\n",
    "\n",
    "In this notebook, we examplify how to use `justcause` in order to evaluate methods using reference datasets. For simplicity, we only use one dataset, but show how evaluation works with multiple methods. Both standard causal methods implemented in the framework as well as custom methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom First\n",
    "The goal of the `justcause` framework is to be a modular and flexible facilitator of causal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/MaximilianFranz/anaconda3/envs/justcause/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# Loading all required packages \n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from justcause.data import Col\n",
    "from justcause.data.sets import load_ihdp\n",
    "from justcause.learners import SLearner\n",
    "from justcause.metrics import pehe_score, mean_absolute\n",
    "from justcause.evaluation import setup_result_df, setup_scores_df, calc_scores, \\\n",
    "    summarize_scores\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data and methods you want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_ihdp()\n",
    "metrics = [pehe_score, mean_absolute]\n",
    "\n",
    "# Limit evaluation to the first 100 replications of IHDP\n",
    "replications = list(itertools.islice(data, 100))\n",
    "train_size = 0.8\n",
    "random_state = 42\n",
    "\n",
    "def slearner_eval(train, test):\n",
    "    \"\"\"\n",
    "    Custom method that takes 'train' and 'test' CausalFrames (see causal_frames.ipynb)\n",
    "    and returns ITE predictions for both after training on 'train'. \n",
    "    \n",
    "    Implement your own method in a similar fashion to evaluate them within the framework!\n",
    "    \"\"\"\n",
    "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
    "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
    "\n",
    "    slearner = SLearner(LinearRegression())\n",
    "    slearner.fit(train_X, train_t, train_y)\n",
    "    return (\n",
    "        slearner.predict_ite(train_X, train_t, train_y),\n",
    "        slearner.predict_ite(test_X, test_t, test_y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Evaluation Loop\n",
    "Now given a callable like `slearner_eval` we can evaluate that method using multiple metrics on the given replications. \n",
    "The result dataframe then contains two rows with the summarized scores over all replications for train and test separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = setup_result_df(metrics)\n",
    "    \n",
    "test_scores = setup_scores_df(metrics)\n",
    "train_scores = setup_scores_df(metrics)\n",
    "\n",
    "for rep in replications:\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        rep, train_size=train_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # REPLACE this with the function you implemented and want to evaluate\n",
    "    train_ite, test_ite = slearner_eval(train, test)\n",
    "\n",
    "    # Calculate the scores and append them to a dataframe\n",
    "    test_scores.loc[len(test_scores)] = calc_scores(test[Col.ite],\n",
    "                                                    test_ite,\n",
    "                                                    metrics)\n",
    "\n",
    "    train_scores.loc[len(train_scores)] = calc_scores(train[Col.ite],\n",
    "                                                    train_ite,\n",
    "                                                    metrics)\n",
    "\n",
    "# Summarize the scores and save them in a dataframe\n",
    "results_df.loc[len(results_df)] = np.append(['slearner', True], summarize_scores(train_scores))\n",
    "results_df.loc[len(results_df)] = np.append(['slearner', False], summarize_scores(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this has hardly any advantages if only one dataset and one method is used, because you might as well just implement all the evaluation manually. However, this can simply be expanded to more methods by looping over the callables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_eval(train, test):\n",
    "    \"\"\" Nonesense weighted SLearner evaluation \"\"\"\n",
    "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
    "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
    "\n",
    "    slearner = SLearner(LinearRegression())\n",
    "    slearner.fit(train_X, train_t, train_y, weights=np.full(len(train_t), 1))\n",
    "    return (\n",
    "        slearner.predict_ite(train_X, train_t, train_y),\n",
    "        slearner.predict_ite(test_X, test_t, test_y)\n",
    "    )\n",
    "\n",
    "methods = [another_eval, slearner_eval]\n",
    "\n",
    "results_df = setup_result_df(metrics)\n",
    "\n",
    "for method in methods:\n",
    "    \n",
    "    test_scores = setup_scores_df(metrics)\n",
    "    train_scores = setup_scores_df(metrics)\n",
    "\n",
    "    for rep in replications:\n",
    "\n",
    "        train, test = train_test_split(\n",
    "            rep, train_size=train_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # REPLACE this with the function you implemented and want to evaluate\n",
    "        train_ite, test_ite = method(train, test)\n",
    "\n",
    "        # Calculate the scores and append them to a dataframe\n",
    "        test_scores.loc[len(test_scores)] = calc_scores(test[Col.ite],\n",
    "                                                        test_ite,\n",
    "                                                        metrics)\n",
    "\n",
    "        train_scores.loc[len(train_scores)] = calc_scores(train[Col.ite],\n",
    "                                                        train_ite,\n",
    "                                                        metrics)\n",
    "\n",
    "    # Summarize the scores and save them in a dataframe\n",
    "    results_df.loc[len(results_df)] = np.append([method.__name__, True], summarize_scores(train_scores))\n",
    "    results_df.loc[len(results_df)] = np.append([method.__name__, False], summarize_scores(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.625971000721637</td>\n",
       "      <td>2.6359926738390502</td>\n",
       "      <td>8.213625971533043</td>\n",
       "      <td>1.2926681149657069</td>\n",
       "      <td>0.39624557185266385</td>\n",
       "      <td>2.4746034286861276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.625971000721637</td>\n",
       "      <td>2.6359926738390502</td>\n",
       "      <td>8.213625971533043</td>\n",
       "      <td>1.2926681149657069</td>\n",
       "      <td>0.39624557185266385</td>\n",
       "      <td>2.4746034286861276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          method  train    pehe_score-mean   pehe_score-median  \\\n",
       "0   another_eval   True  5.633659795888926   2.623297102872905   \n",
       "1   another_eval  False  5.625971000721637  2.6359926738390502   \n",
       "2  slearner_eval   True  5.633659795888926   2.623297102872905   \n",
       "3  slearner_eval  False  5.625971000721637  2.6359926738390502   \n",
       "\n",
       "      pehe_score-std  mean_absolute-mean mean_absolute-median  \\\n",
       "0  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "1  8.213625971533043  1.2926681149657069  0.39624557185266385   \n",
       "2  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "3  8.213625971533043  1.2926681149657069  0.39624557185266385   \n",
       "\n",
       "    mean_absolute-std  \n",
       "0  1.4932757697867245  \n",
       "1  2.4746034286861276  \n",
       "2  1.4932757697867245  \n",
       "3  2.4746034286861276  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because in most cases, we're not changing anything within this loop expect the way `train_ite` and `test_ite` are calculated based on `train` and `test`, `justcause` provides a default implementation of that loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TLearner' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-565dcb118cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjustcause\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_ite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ba/eval/justcause/src/justcause/evaluation.py\u001b[0m in \u001b[0;36mevaluate_ite\u001b[0;34m(replications, methods, metrics, formats, train_size, random_state)\u001b[0m\n\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TLearner' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "from justcause.evaluation import evaluate_ite\n",
    "\n",
    "result = evaluate_ite(replications, methods, metrics, train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          method  train    pehe_score-mean  pehe_score-median  \\\n",
       "0   another_eval   True  5.633659795888926  2.623297102872905   \n",
       "1   another_eval  False  5.633659795888926  2.623297102872905   \n",
       "2  slearner_eval   True  5.633659795888926  2.623297102872905   \n",
       "3  slearner_eval  False  5.633659795888926  2.623297102872905   \n",
       "\n",
       "      pehe_score-std  mean_absolute-mean mean_absolute-median  \\\n",
       "0  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "1  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "2  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "3  8.362124759175456  0.7324426200135632  0.23818504313199274   \n",
       "\n",
       "    mean_absolute-std  \n",
       "0  1.4932757697867245  \n",
       "1  1.4932757697867245  \n",
       "2  1.4932757697867245  \n",
       "3  1.4932757697867245  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding standard causal methods to the mix\n",
    "Within `justcause.learners` we've implemented a couple of standard methods that provide a `predict_ite()` method. Instead of going the tedious way like we've done in `slearner_eval` above, we can just use these methods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justcause.learners import TLearner, XLearner, RLearner\n",
    "\n",
    "# All in standard configuration\n",
    "methods = [another_eval, slearner_eval, TLearner(), XLearner(), RLearner(LinearRegression())]\n",
    "\n",
    "result = evaluate_ite(replications, methods, metrics, train_size=train_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>train</th>\n",
       "      <th>pehe_score-mean</th>\n",
       "      <th>pehe_score-median</th>\n",
       "      <th>pehe_score-std</th>\n",
       "      <th>mean_absolute-mean</th>\n",
       "      <th>mean_absolute-median</th>\n",
       "      <th>mean_absolute-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>another_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>True</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>slearner_eval</td>\n",
       "      <td>False</td>\n",
       "      <td>5.633659795888926</td>\n",
       "      <td>2.623297102872905</td>\n",
       "      <td>8.362124759175456</td>\n",
       "      <td>0.7324426200135632</td>\n",
       "      <td>0.23818504313199274</td>\n",
       "      <td>1.4932757697867245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TLearner(control=LassoLars, treated=LassoLars)</td>\n",
       "      <td>True</td>\n",
       "      <td>5.5726257778279535</td>\n",
       "      <td>2.5437982727262867</td>\n",
       "      <td>8.213573470353799</td>\n",
       "      <td>0.2931874178151998</td>\n",
       "      <td>0.16637003903539105</td>\n",
       "      <td>0.4280283924070575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TLearner(control=LassoLars, treated=LassoLars)</td>\n",
       "      <td>False</td>\n",
       "      <td>5.5726257778279535</td>\n",
       "      <td>2.5437982727262867</td>\n",
       "      <td>8.213573470353799</td>\n",
       "      <td>0.2931874178151998</td>\n",
       "      <td>0.16637003903539105</td>\n",
       "      <td>0.4280283924070575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XLearner(outcome_c=LassoLars, outcome_t=LassoL...</td>\n",
       "      <td>True</td>\n",
       "      <td>5.579284802381838</td>\n",
       "      <td>2.5437982727262867</td>\n",
       "      <td>8.24060645800564</td>\n",
       "      <td>0.2896989907685176</td>\n",
       "      <td>0.1663700390353935</td>\n",
       "      <td>0.42700784707898404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XLearner(outcome_c=LassoLars, outcome_t=LassoL...</td>\n",
       "      <td>False</td>\n",
       "      <td>5.579284802381838</td>\n",
       "      <td>2.5437982727262867</td>\n",
       "      <td>8.24060645800564</td>\n",
       "      <td>0.2896989907685176</td>\n",
       "      <td>0.1663700390353935</td>\n",
       "      <td>0.42700784707898404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RLearner(outcome=LinearRegression, effect=Line...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.556630302290813</td>\n",
       "      <td>1.2271614300772207</td>\n",
       "      <td>3.7499331919781107</td>\n",
       "      <td>0.25846876877559305</td>\n",
       "      <td>0.17986988119561786</td>\n",
       "      <td>0.30684648153787425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RLearner(outcome=LinearRegression, effect=Line...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.556630302290813</td>\n",
       "      <td>1.2271614300772207</td>\n",
       "      <td>3.7499331919781107</td>\n",
       "      <td>0.25846876877559305</td>\n",
       "      <td>0.17986988119561786</td>\n",
       "      <td>0.30684648153787425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              method  train  \\\n",
       "0                                       another_eval   True   \n",
       "1                                       another_eval  False   \n",
       "2                                      slearner_eval   True   \n",
       "3                                      slearner_eval  False   \n",
       "4     TLearner(control=LassoLars, treated=LassoLars)   True   \n",
       "5     TLearner(control=LassoLars, treated=LassoLars)  False   \n",
       "6  XLearner(outcome_c=LassoLars, outcome_t=LassoL...   True   \n",
       "7  XLearner(outcome_c=LassoLars, outcome_t=LassoL...  False   \n",
       "8  RLearner(outcome=LinearRegression, effect=Line...   True   \n",
       "9  RLearner(outcome=LinearRegression, effect=Line...  False   \n",
       "\n",
       "      pehe_score-mean   pehe_score-median      pehe_score-std  \\\n",
       "0   5.633659795888926   2.623297102872905   8.362124759175456   \n",
       "1   5.633659795888926   2.623297102872905   8.362124759175456   \n",
       "2   5.633659795888926   2.623297102872905   8.362124759175456   \n",
       "3   5.633659795888926   2.623297102872905   8.362124759175456   \n",
       "4  5.5726257778279535  2.5437982727262867   8.213573470353799   \n",
       "5  5.5726257778279535  2.5437982727262867   8.213573470353799   \n",
       "6   5.579284802381838  2.5437982727262867    8.24060645800564   \n",
       "7   5.579284802381838  2.5437982727262867    8.24060645800564   \n",
       "8   2.556630302290813  1.2271614300772207  3.7499331919781107   \n",
       "9   2.556630302290813  1.2271614300772207  3.7499331919781107   \n",
       "\n",
       "    mean_absolute-mean mean_absolute-median    mean_absolute-std  \n",
       "0   0.7324426200135632  0.23818504313199274   1.4932757697867245  \n",
       "1   0.7324426200135632  0.23818504313199274   1.4932757697867245  \n",
       "2   0.7324426200135632  0.23818504313199274   1.4932757697867245  \n",
       "3   0.7324426200135632  0.23818504313199274   1.4932757697867245  \n",
       "4   0.2931874178151998  0.16637003903539105   0.4280283924070575  \n",
       "5   0.2931874178151998  0.16637003903539105   0.4280283924070575  \n",
       "6   0.2896989907685176   0.1663700390353935  0.42700784707898404  \n",
       "7   0.2896989907685176   0.1663700390353935  0.42700784707898404  \n",
       "8  0.25846876877559305  0.17986988119561786  0.30684648153787425  \n",
       "9  0.25846876877559305  0.17986988119561786  0.30684648153787425  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
